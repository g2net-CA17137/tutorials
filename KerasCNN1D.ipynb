{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPool1D, Flatten, SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "# General packages\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data preparation and validation packages\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Jupyter interactive plotting\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callback functions\n",
    "\n",
    "Callback functions, as the name suggests, are type of functions that are called by particular part of your code the moment it executes and process the data. In the following case I created the simple class than can be used to plot the value of loss function and accuracy during the training/validation of the model.\n",
    "\n",
    "It will be called by `fit` method of `Sequential` object in order to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real time plotting\n",
    "class PlotLosses(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accuracies.append(logs.get('acc'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.val_accuracies.append(logs.get('val_acc'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.accuracies, label=\"accuracy\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.plot(self.x, self.val_accuracies, label=\"val_accuracy\")\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "Here I prepare the data for the model as was described in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.path.dirname(os.getcwd()),\"data\")\n",
    "hdf5_filename = \"example.hdf5\"\n",
    "h5=h5py.File(os.path.join(data_dir,hdf5_filename), 'r')\n",
    "strain = np.array(h5[\"Strain\"][\"Strain\"].value)\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make artificial dataset\n",
    "data = np.zeros((1000, strain.shape[0]))\n",
    "data[:] = strain\n",
    "rescaled_data = preprocessing.minmax_scale(data.T).T\n",
    "rescaled_data = np.reshape(rescaled_data, (rescaled_data.shape[0], rescaled_data.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make artificial labels - 3 classes\n",
    "labels = np.random.randint(0, 3, rescaled_data.shape[0])\n",
    "n_classes = labels.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's shuffle data\n",
    "ind = np.random.permutation(rescaled_data.shape[0])\n",
    "rescaled_data = np.take(rescaled_data, ind, axis=0)\n",
    "labels = np.take(labels, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert labels into one hot encoding\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "labels = labels.reshape(labels.shape[0], 1)\n",
    "targets = onehot_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data for training and validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(rescaled_data, targets, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data architecture\n",
    "\n",
    "**Important note!** Keras can work either with Tensorflow background or Theano. They differ in one crucial aspect - shape of the data.\n",
    "\n",
    "Tensorflow requires the data to be stored in the following way (for 1D CNN):\n",
    "\n",
    "- n_samples, n_features, n_channels\n",
    "\n",
    "Whereas Theano requires:\n",
    "\n",
    "- n_samples, n_channels, n_features\n",
    "\n",
    "Channel in case of 2D CNN denotes to amount of colors on the image but in case of 1D it can be used to express one feature based on two datasets.\n",
    "\n",
    "Make sure that the shape of the data is correct with respect to the Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(nb_filter=40, filter_length=3, activation=\"relu\", input_shape=(data.shape[1],1)))\n",
    "model.add(MaxPool1D(2))\n",
    "#model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "model.add(Conv1D(nb_filter=20, filter_length=3, activation=\"relu\"))\n",
    "model.add(MaxPool1D(2))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "# Flatten your convolutional part to fit the dense part of the model\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(200, activation=\"relu\"))\n",
    "model.add(Dense(n_classes, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                epochs=5,\n",
    "                batch_size=64,\n",
    "                validation_data=(x_val, y_val),\n",
    "                callbacks=[plot_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = confusion_matrix(y_val.argmax(1), y_predicted.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = np.trace(cms) / np.sum(cms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 14))\n",
    "ax = fig.add_subplot(111)\n",
    "im = ax.imshow(np.transpose(cms), interpolation=\"nearest\", cmap=\"cool\")\n",
    "rows = cms.shape[0]\n",
    "cols = cms.shape[1]\n",
    "for x in range(0, rows):\n",
    "    for y in range(0, cols):\n",
    "        value = int(cms[x, y])\n",
    "        ax.text(x, y, value, color=\"black\", ha=\"center\", va=\"center\", fontsize=25)\n",
    "plt.title(\"Real vs predicted data, accuracy: \" + str(test_score), fontsize=25)\n",
    "plt.colorbar(im)\n",
    "\n",
    "classes_values = []\n",
    "classes_labels = []\n",
    "for n in range(n_classes):\n",
    "    classes_values.append(n)\n",
    "    classes_labels.append(str(n))\n",
    "\n",
    "#plt.xticks([0, 1, 2, 3, 4], [\"0\", \"1\", \"2\", \"3\", \"4\"], rotation=45, fontsize=25)\n",
    "#plt.yticks([0, 1, 2, 3, 4], [\"0\", \"1\", \"2\", \"3\", \"4\"], fontsize=25)\n",
    "plt.xticks(classes_values, classes_labels, rotation=45, fontsize=25)\n",
    "plt.yticks(classes_values, classes_labels, fontsize=25)\n",
    "plt.xlabel(\"Real data\", fontsize=25)\n",
    "plt.ylabel(\"Predicted data\", fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
